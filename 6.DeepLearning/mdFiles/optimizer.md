# Optimizer
딥러닝은 입력층과 출력층 사이에 여러 개의 은닉층으로 이루어진 신경망이다.  
   
층이 깊어질수록 모듈과 함수에 따른 하이퍼파라미터도 비례하여 많아지기에,  
이 **하이퍼파리미터를 결정**하여 **모델이 정확하게 결과**를 뱉어낼 수 있도록 하는 것이 **학습**의 핵심이다.  
그러기 위해 loss function을 정의하여야 하며, 이를 최적화 해야한다.  
   
딥러닝 학습시 최대한 틀리지 않는 방향으로 학습해야 한다.  
얼마나 틀린지(loss)를 알게 하는 함수가 loss function(손실함수)이다.  
loss function의 최솟값을 찾는 것을 학습 목표로 한다.  
   
최솟값을 찾아가는 것을 **최적화 = optimization** 이라하며,  
이를 수행하는 알고리즘이 **최적화 알고리즘 = optimizer**이다.  
   
즉, 학습 속도를 빠르고 안정적이게 최적화 하는 것을 말한다고 할 수 있다.

   
- Gradient Descent (경사하강법) : 함수의 기울기(경사)를 구하여 함수의 극값에 이를 때까지 기울기가 낮은 쪽으로 반복하여 이동하는 방법.  
    ⇒ SGD (batch 학습)  
- Momentum (관성) : 이전에 이동했던 방향을 기억해서 다음 이동의 방향에 반영  
    ⇒ NAG  
- Adagrad (Adaptive Gradient) : 많이 이동한 변수(w)는 최적값에 근접했을 것이라는 가정하에 , 많이 이동한 변수(w)를 기억해서 다음 이동의 거리를 줄인다.  
    ⇒ RMSprop  
- Adam (RMSprop + Momentum) : 진행하던 속도에 **관성**을 주고, 최근 경로의 곡면의 변화량에 따른 **적응적 학습률**을 갖는 알고리즘  