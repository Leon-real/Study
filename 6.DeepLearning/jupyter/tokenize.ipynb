{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182f700f",
   "metadata": {},
   "source": [
    "# Tokenize 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8106b93",
   "metadata": {},
   "source": [
    "```\n",
    "text = \"아들아 너는 계획이 다\"\n",
    "tokens = 토큰화.숫자로(text)\n",
    "# [ 2656, 47485, 1681, 47441, 10635, 148]\n",
    "outputs = 다음토큰(tokens) # 결과: 나올 수 있는 모든 토큰의\n",
    "확률값\n",
    "token = 가장높은확률의토큰(outputs)\n",
    "# 605\n",
    "decoded = 토큰화.단어로(token)\n",
    "# 있어\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839b8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef02983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f2f6f1c7ec4f70b72b5c339b7b466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b5d11131c647368b0cab161cfc82fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa7259e957747b9b0412164f9ca03e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ca5bc233b54d76804d2150dbae95e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dc011c67c342758fbd8c7a636d6d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "# 토큰화 함수\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"taeminlee/kogpt2\")\n",
    "# 다음 단어 함수\n",
    "model = GPT2LMHeadModel.from_pretrained(\"taeminlee/kogpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24fa9f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2656, 47485,  1681, 47441, 10635,   148,   605]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"아들아 너는 계획이 다 있어\"\n",
    "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4925ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 50000])\n",
      "tensor([  0.8397,  10.9158, -12.0285,  ...,  -5.1338,  -1.9830,  -1.5586],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor(6.1770, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(tokens) # 모든 토큰들의 확률값\n",
    "print(outputs[0].shape)\n",
    "outputs = outputs[0][0, -1, :]\n",
    "print(outputs)\n",
    "print(outputs[605])\n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a9ca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) </s>\n"
     ]
    }
   ],
   "source": [
    "token = outputs.argmax(-1) # 가장 높은 확률의 토큰\n",
    "decoded = tokenizer.decode(token)\n",
    "print(token, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4d9e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 운데\n",
      "601 되었다\n",
      "602 최고\n",
      "603 기에\n",
      "604 항\n",
      "605 있어\n",
      "606 이라는\n",
      "607 넘\n",
      "608 폭\n",
      "609 번\n"
     ]
    }
   ],
   "source": [
    "for t in range(600,610):\n",
    "    print(t, tokenizer.decode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c802d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2656, 47485,  1681, 47441, 10635,   148,   605,     1,     0,   155,\n",
      "          872,  7892,  8274,   605,     1,     0,   155,   872,  7892,  8274])\n",
      "아들아 너는 계획이 다 있어</s><s> 나 지금 집에 가고 있어</s><s> 나 지금 집에 가고\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "# 토큰화 함수\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"taeminlee/kogpt2\")\n",
    "# 다음 단어 함수\n",
    "model = GPT2LMHeadModel.from_pretrained(\"taeminlee/kogpt2\")\n",
    "text = \"아들아 너는 계획이 다\"\n",
    "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(tokens)\n",
    "print(outputs[0])\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78113175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1050f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250b96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b02fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc09578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7d998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
