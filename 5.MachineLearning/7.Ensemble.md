# 앙상블 알고리즘 등

## 앙상블(Ensemble)

- 통합은 힘이다.
- 약한 모델이 올바르게 결합하면 더 정확하고 견고한 모델을 얻을 수 있다.

## 보팅(voting)

- 여러 모델들(다른 유형의 알고리즘 기반)의 예측 결과를 투표를 통해 최종 예측 결과를 결정하는 방법
- 하드 보팅 : 다수 모델이 예측한 값이 최종 결과값
- 소프트 보팅 : 모든 모델이 예측한 레이블 값의 결정 확률 평균을 구한 뒤, 가장 확률이 높은 값을 최종 선택

## 배깅(Bagging)

- Bootstrap Aggregating의 약자
- 데이터로부터 부트스트랩 한 데이터로 모델들을 학습시킨 후, 모델들의 예측 결과를 집계해 최종 결과를 얻는 방법
- 같은 유형의 알고리즘 기반 모델들을 사용
- 데이터 분할 시 중복을 허용
- 범주형 데이터는 투표(보팅voting) 방식으로 겨과를 집계
- 연속형 데이터는 평균으로 결과를 집계
- 대표적인 배깅 알고리즘 : Random Forest

```python
# Random Forest - 회귀모델 구현
# 알고리즘 함수: sklearn.ensemble.RandomForestRegressor
# 성능평가 함수: sklearn.metrics.mean_absolute_error, sklearn.metrics.r2_score 등

# 불러오기
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
# 선언하기
model = RandomForestRegressor(max_depth=5, n_estimators=100, random_state=1)
# 학습하기
model.fit(x_train, y_train)
# 예측하기
y_pred = model.predict(x_test)
# 평가하기
print(mean_absolute_error(y_test, y_pred))
print(r2_score(y_test, y_pred))
```

```python
# Random Forest - 분류모델 구현
# 알고리즘 함수: sklearn.ensemble.RandomForestClassifier
# 성능평가 함수: sklearn.metrics.confusion_matrix, sklearn.metrics.classification_report 등

# 불러오기
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report
# 선언하기
model = RandomForestClassifier(max_depth=5, n_estimators=100, random_state=1)
# 학습하기
model.fit(x_train, y_train)
# 예측하기
y_pred = model.predict(x_test)
# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

## 부스팅(Boosting)

- 같은 유형의 알고리즘 기반 모델 여러 개에 대해 순차적으로 학습을 수행한다.
- 이전 모델이 제대로 예측하지 못한 데이터에 대해서 가중치를 부야하여 다음 모델이 학습과 예측을 진행하는 방법
- 계속하여 모델에 가중치를 부스팅하며 학습을 진행해 부스팅 방식이라고 한다.
- 예측 성능이 뛰어나 앙상블 학습을 주도한다.
- 배깅에 비해 성능이 좋지만, 속도가 느리고 과적합 발생 가능성이 있다.  
→ 상황에 맞게 적절히 사용해야 한다.
- 대표적인 부스팅 알고리즘 : XGBoost, LightGBM

```python
# 회귀모델 구현
# 알고리즘 함수: xgboost.XGBRegressor
# 성능평가 함수: sklearn.metrics.mean_absolute_error, sklearn.metrics.r2_score 등

# 불러오기
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, r2_score
# 선언하기
model = XGBRegressor(max_depth=5, n_estimators=100, random_state=1)
# 학습하기
model.fit(x_train, y_train)
# 예측하기
y_pred = model.predict(x_test)
# 평가하기
print(mean_absolute_error(y_test, y_pred))
print(r2_score(y_test, y_pred))
```

```python
# 분류모델 구현
# 알고리즘 함수: xgboost.XGBClassifier
# 성능평가 함수: sklearn.metrics.confusion_matrix, sklearn.metrics.classification_report 등

# 불러오기
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report
# 선언하기
model = XGBClassifier(max_depth=5, n_estimators=100, random_state=1)
# 학습하기
model.fit(x_train, y_train)
# 예측하기
y_pred = model.predict(x_test)
# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

## 스태킹(Stacking)

- 여러 모델의 예측 값을 최종 모델의 학습 데이터로 사용하여 예측하는 방법
- 현실 모델에서 많이 사용되지 않으며, 캐글 같은 미세한 성능 차이로 승부를 결정하는 대회에서 사용된다.
- 기본 모델로 4개 이상 선택해야 좋은 결과를 기대할 수 있다.