# 기본 알고리즘 원리

## Linear Regression
- **회귀 모델**에만 사용할 수 있다.
    
    ```python
    # 회귀모델 구현
    
    # 불러오기
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_absolute_error, r2_score
    # 선언하기
    model = LinearRegression()
    # 학습하기
    model.fit(x_train, y_train)
    # 예측하기
    y_pred = model.predict(x_test)
    # 평가하기
    print(mean_absolute_error(y_test, y_pred))
    print(r2_score(y_test, y_pred))
    ```
    
- 평균으로 돌아가려는 경향이 있다는 가설을 바탕으로 분석하는 방법
- 최선의 회귀 모델 → 전체 데이터의 **오차 합이 최소**가 되는 모델
- 단순 회귀 : 독립 변수 하나가 종속 변수에 영향을 미치는 선형 회귀
- 다중 회귀 : 여러 독립 변수가 종속 변수에 영향을 미치는 선형 회귀
## K-Nearest Neighbor
- **회귀**와 **분류**에 사용되는 간단한 지도 학습 알고리즘
- K개 값의 평균을 계산하여 값을 예측한다.(회귀)  
- k값이 많아질수록 모델은 단순해진다.(왜냐하면 K값이 많아질수록 평균에 가까워지므로)  
- 가장 많이 포함된 유형으로 분류한다.(분류)  

    - 회귀
        - 알고리즘 함수: sklearn.neighbors.KNeighborsRegressor
        - 성능평가 함수: sklearn.metrics.mean_absolute_error, sklearn.metrics.r2_score 등
            
            ```python
            # 회귀모델 구현
            # 불러오기
            from sklearn.neighbors import KNeighborsRegressor
            from sklearn.metrics import mean_absolute_error, r2_score
            # 선언하기
            model = KNeighborsRegressor(n_neighbors=5)
            # 학습하기
            model.fit(x_train, y_train)
            # 예측하기
            y_pred = model.predict(x_test)
            # 평가하기
            print(mean_absolute_error(y_test, y_pred))
            print(r2_score(y_test, y_pred))
            ```
            
    - 분류
        - 알고리즘 함수: sklearn.neighbors.KNeighborsClassifier
        - 성능평가 함수: sklearn.metrics.confusion_matrix, sklearn.metrics.classification_report등
            
            ```python
            # 분류모델 구현
            # 불러오기
            from sklearn.neighbors import KNeighborsClassifier
            from sklearn.metrics import confusion_matrix, classification_report
            # 선언하기
            model = KNeighborsClassifier(n_neighbors=5)
            # 학습하기
            model.fit(x_train, y_train)
            # 예측하기
            y_pred = model.predict(x_test)
            # 평가하기
            print(confusion_matrix(y_test, y_pred))
            print(classification_report(y_test, y_pred))
            ```
            
- 학습용 데이터에서 k개의 최근접 이웃의 값을 찾아 그 값들로 새로운 값을 예측하는 알고리즘
- 연산 속도는 느리다.
- k(탐색하는 이웃 개수)에 따라 데이터를 다르게 예측할 수 있다. 따라서 **적절한 K값**을 찾는 것이 중요하다.  
    - 대표적인 스케일링 방법  
        - **주의 : 평가용 데이터도 학습용 데이터를 기준으로 스케일링을 수행한다.**  
        - 정규화 : 각 변수의 값이 0과 1 사이 값이 됌  
            $\large X_{norm}= \frac {x-x_{min}} {x_{max}-x_{min}}$  
        - 표준화 : 각 변수의 평균이 0, 표준편차가 1이 됌  
            $\large X_{z}= \frac {x-x_{mean}} {x_{std}}$  
- 스케일링 여부에 따라 KNN 모델 성능이 달라질 수 있다.
    
    ```python
    # 대표적인 스케일링 방법
    # 함수 불러오기
    from sklearn.preprocessing import MinMaxScaler
    # 정규화
    scaler = MinMaxScaler()
    scaler.fit(x_train)
    x_train = scaler.transform(x_train)
    x_test = scaler.transform(x_test)
    ```
    
## Decision Tree
- **분류**와 **회귀** 모두에 사용되는 지도 학습 알고리즘
    - 회귀
        - 비용함수 ⇒ MSE  
        - 알고리즘 함수: sklearn.tree.DecisionTreeRegressor
        - 성능평가 함수: sklearn.metrics.mean_absolute_error, sklearn.metrics.r2_score 등
        
        ```python
        # 불러오기
        from sklearn.tree import DecisionTreeRegressor
        from sklearn.metrics import mean_absolute_error, r2_score
        # 선언하기
        model = DecisionTreeRegressor(max_depth=5)
        # 학습하기
        model.fit(x_train, y_train)
        # 예측하기
        y_pred = model.predict(x_test)
        # 평가하기
        print(mean_absolute_error(y_test, y_pred))
        print(r2_score(y_test, y_pred))
        ```
        
    - 분류
        - 비용함수 ⇒ 불순도(불순도가 낮고, 순도가 높을수록 분류가 잘 된 것)   
        - 알고리즘 함수: sklearn.tree.DecisionTreeClassifier
        - 성능평가 함수: sklearn.metrics.confusion_matrix, sklearn.metrics.classification_report등
        
        ```python
        # 불러오기
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import confusion_matrix, classification_report
        # 선언하기
        model = DecisionTreeClassifier(max_depth=5)
        # 학습하기
        model.fit(x_train, y_train)
        # 예측하기
        y_pred = model.predict(x_test)
        # 평가하기
        print(confusion_matrix(y_test, y_pred))
        print(classification_report(y_test, y_pred))
        ```
        
- 분석 과정이 직관적이며, 이해와 설명하기가 쉽다.
- 스케일링 등의 전처리 영향도가 크지 않다.
- **의미 있는 질문을 먼저 하는 것이 중요**하다.
- 확률에 근거해 예측을 하는 것이다.

```
Decision Tree 용어 설명

- Root Node(뿌리 마디): 전체 자료를 갖는 시작하는 마디
- Child Node(자식 마디): 마디 하나로부터 분리된 2개 이상의 마디
- Parent Node(부모 마디): 주어진 마디의 상위 마디
- Terminal Node(끝 마디): 자식 마디가 없는 마디(=Leaf Node)
- Internal Node(중간 마디): 부모 마디와 자식 마디가 모두 있는 마디
- Branch(가지): 연결되어 있는 2개 이상의 마디 집합
- Depth(깊이): 뿌리 마디로부터 끝 마디까지 연결된 마디 개수(오른쪽 트리의 경우 5)
```

## Logistic Regression
- **분류** 모델에만 사용할 수 있다.
- 로지스틱 함수(시그모이드 함수)  
$\large p = \frac {1} {1 + e^{-f(x)}}$  
    - 알고리즘 함수: sklearn.linear_model.LogisticRegression
    - 성능평가 함수: sklearn.metrics.confusion_matrix, sklearn.metrics.classification_report 등
    
    ```python
    # 불러오기
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import confusion_matrix, classification_report
    # 선언하기
    model = LogisticRegression()
    # 학습하기
    model.fit(x_train, y_train)
    # 예측하기
    y_pred = model.predict(x_test)
    # 평가하기
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    ```
    
- 로지스틱 함수 → 시그모이드 함수라고도 부른다.
- 확률 값 p는 선형 판별식 값이 커지면 1, 작아지면 0에 가까운 값이 된다.
- (-∞, ∞)범위를 갖는 선형 판별식 결과로 (0,1) 범위의 확률 값을 얻게 된다.
- 기본적으로 확률 값 0.5를 임계값으로 하여, 이보다 크면 1, 아니면 0으로 분류함  